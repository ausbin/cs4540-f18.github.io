# Course Info

**Professor:** Jacob Abernethy<br>
**TAs:** Benjamin Bray and Naveen Kodali<br>
**Location:** Tu/Th 3-4:15pm in Klaus 2443

[Please fill out the COURSE SURVEY!](https://goo.gl/forms/LRJRu5DLhkLfmMCt1)

This is an advanced course on algorithms. That is quite a broad topic, and in particular this semester’s course will focus heavily on **algorithms for machine learning**. We will be especially interested in diving into the following topics:

* Numerical Analysis
* Convex Geometry & Optimization
* Probability & Statistical Inference

While students should have a strong back background in core algorithmic concepts, linear algebra, calculus, and probability, we will review many of these topics early in the course. Students will be required to write code in Python, and we will present much of the material in the course using [Jupyter Notebooks](http://jupyter.org/).

## Hands-on Format

In most courses, students learn about material in class through lecture, and then they practice problem solving on their own by doing homework.  In this course we will do the opposite! Students will be required to read material before each class period, and then arrive in class ready to dive into problem-solving.  This way, students can familiarize themselves with basic definitions and examples at home, and benefit from one-on-one interaction with the course staff during lecture while working through more challenging aspects of the material.  Lecture notes for each day will be posted online at least one week prior to each lecture (with the first week as an exception).

Why do it like this? The lecture format is an outdated way to teach mathematical material, especially for topics such as algorithms and machine learning where it is so easy to play with code and implement ideas. The lecture format also limits the professor’s ability to interact directly with students.

Each class period will have the following structure:

* *(0mins)* Students arrive and organize themselves by sitting with their group
* *(5mins)* Students take a very short and simple quiz on assigned reading material
* *(55mins)* Problem time! Instructor presents a sequence of problems, and students are given 5-10 minutes per problem to try to solve each one with their group. Instructors will move around the classroom to engage with students and answer questions.
* *(15mins)* Class wraps with a brief description of the forthcoming material for next period, in a “mini lecture”

## Grading

The course grades will be determined as follows.  Note that the in-class quizzes will be graded generously, and 50% of the credit will be given simply for showing up and answering the questions.

| Homeworks		| 35% |
| Attendance + Quizzes	| 15% |
| Midterm Exam		| 20% |
| Final Exam		| 30% |

Quizzes will be entered electronically, via a web form, so make sure you have a phone, laptop, or tablet with you in class!  If you don’t have access to any of these, please let the instructor know.  The grading scheme will be:

* 2 points for a correct answer
* 1 point for an incorrect answer
* 0 points for not taking the quiz

Your **five lowest quiz scores will be dropped**, which should be enough to account for quizzes missed due to planned or unplanned absences.

Students are allowed, and indeed encouraged, to work on homework with other students in the course. But when solutions are written up, this should be done alone and without the help of other students. Students are required to specify on their writeups which students that collaborated with. If we find solutions that appear even remotely to have been copied, these will be given a zero and the students will be notified.

## Reading

Readings will be assigned for you to complete *before each lecture*.  All required reading will either be linked to here or posted to canvas.  You are not required to purchase a textbook for this course, but you may find the following books helpful.

* Boyd & Vandengerbhe, *Convex Optimization* ([Free PDF](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf))
* Trefethen & Bau, *Numerical Linear Algebra*

## Homework

Coming soon!

# Schedule

(Tu 8/21/18) **Lecture #1:  Introduction & Perceptron**

* CMU 15-859(B), Lecture #4, [The Perceptron Algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf) by Avrim Blum
* Raul Rojas, Neural Networks:  A Systematic Introduction
  * [Ch 4:  Perceptron Learning](https://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf)
  * [Ch 5:  Unsupervised Learning and Clustering Algorithms](https://page.mi.fu-berlin.de/rojas/neural/chapter/K5.pdf)
  
(Th 8/23/18) **Lecture #2:  Review of Linear Algebra & Intro to Numpy**

Required Reading (before class!)
* Notes, [CS 4540:  Python Basics](https://cs4540-f18.github.io/notes/python-basics)

Additional Resources
* MIT OCW 18.06 Linear Algebra Lecture Videos

	
(Tu 8/28/18) **Lecture #3:  Review of Calculus & Intro to Optimization**

Required Reading
* Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §2.1 Affine and Convex Sets
	* §2.2 Important Examples (of Affine and Convex Sets)
	* §3.1 Basic Properties and Examples (of Convex Functions)

Additional Resources
* Stanford EE364a, Lecture #2:  Convex Sets ([Slides](http://web.stanford.edu/class/ee364a/lectures/sets.pdf), [Video](https://www.youtube.com/watch?v=P3W_wFZ2kUo))
* Stanford EE364a, Lecture #3:  Convex Functions ([Slides](http://web.stanford.edu/class/ee364a/lectures/functions.pdf), [Video](https://www.youtube.com/watch?v=kcOodzDGV4c))

(Th 8/30/18) **Lecture #4:  Convexity & Optimization**

Required Reading
* Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §2.5 Separating & Supporting Hyperplanes

Additional Resources
* Boyd & Vandenberghe, [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
	* §2.3 Operations that Preserve Convex Sets
	* §3.2 Operations that Preserve Convex Functions
* Jeffe, UIUC Computational Geometry, ["Lecture 1:  Convex Hulls"](http://jeffe.cs.illinois.edu/teaching/compgeom/notes/01-convexhull.pdf)

(Tu 9/4/18) **Lecture #5:  Linear Programming**

Required Reading
* Stanford CS261 Lecture Notes by Tim Roughgarden
  * [Lecture 7: Linear Programming: Introduction and Applications](http://theory.stanford.edu/~tim/w16/l/l7.pdf)
  * [Lecture 8: Linear Programming Duality (Part 1)](http://theory.stanford.edu/~tim/w16/l/l8.pdf)
  * [Lecture 9: Linear Programming Duality (Part 2)](http://theory.stanford.edu/~tim/w16/l/l9.pdf)
