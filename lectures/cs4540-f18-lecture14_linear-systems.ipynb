{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "c11f7b86-dba3-4e8d-afbf-bd147373e999"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np;\n",
    "import scipy;\n",
    "import scipy.misc;\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import demos\n",
    "from code.l12_newton import *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9707fbb8-a773-42d6-982b-ee1f3c961f53"
    }
   },
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d47522cd-d9c5-4612-87a1-e2c1dfdb40d6"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# L14: Solving Linear Systems\n",
    "\n",
    "*Thursday, October 4, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Systems\n",
    "\n",
    "Given $A \\in \\R^{m \\times n}$ and $b \\in \\R^{m}$, want to compute $x \\in \\R^{n}$ such that $Ax = b$.\n",
    "\n",
    "* There are $m$ linear equations and $n$ unknowns\n",
    "* If $m > n$, the system is *overdetermined*\n",
    "* If $m < n$, the system is *underdetermined*\n",
    "* If $m = n$ and $A$ is invertible, we want $x = A^{-1} b$\n",
    "\n",
    "For now, we will focus on **square matrices** $A \\in \\R^{m \\times m}$, which we also assume to be invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Methods\n",
    "\n",
    "You already know of a few algorithms for solving linear systems:\n",
    "\n",
    "* Gradient descent on $f(x) = \\frac{1}{2} x^T A x - b^T x$\n",
    "* Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "68ff68eb-e4c1-4e9d-8e8b-52d509bb70c1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem:  Simple Linear Systems\n",
    "\n",
    "<div style=\"padding:10px;margin:10px;border:1px solid black\">\n",
    "<b>Part A:</b> Solve the diagonal linear system below.  How many operations are required?  Assume all diagonal entries are nonzero.\n",
    "    $$\n",
    "    \\begin{bmatrix} \n",
    "    a_{11} &  &        &                 \\\\\n",
    "            & a_{22}   &        &        \\\\\n",
    "            &        & \\ddots &          \\\\\n",
    "            &        &        & a_{mm}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}\n",
    "    $$\n",
    "</div>\n",
    "<div style=\"padding:10px;margin:10px;border:1px solid black\">\n",
    "<b>Part B:</b> Solve the upper-triangular linear system below.  How many operations are required?  Assume all diagonal entries are nonzero.\n",
    "    $$\n",
    "    \\begin{bmatrix} \n",
    "    a_{11}  & a_{12} & \\cdots & a_{1m} \\\\\n",
    "            & a_{22} & \\cdots & a_{2m} \\\\\n",
    "            &        & \\ddots & \\vdots       \\\\\n",
    "            &        &        & a_{mm}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}\n",
    "    $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "**Part A:** Set $x_k = \\frac{b_k}{a_{kk}}$.  Requires $O(m)$ operations.\n",
    "\n",
    "**Part B:** Use \"back substitution\", requiring $O(m^2)$ operations.  Starting with $x_m = b_m / a_{mm}$, the next equation from the bottom gives\n",
    "    $$\n",
    "    a_{m-1,m-1} x_{m-1} + a_{m-1,m} x_m = b_{m-1} \\\\\n",
    "    \\implies\n",
    "    x_{m-1} = (b_{m-1} - a_{m-1,m} x_m) / a_{m-1,m-1}\n",
    "    $$\n",
    "Continuing in this fashion, we find\n",
    "    $$\n",
    "    x_j = \\frac{1}{a_{jj}} \\left( b_j - \\sum_{k=j+1}^m x_k a_{jk} \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stationary Iterative Methods\n",
    "\n",
    "The idea is to approximate $A^{-1}$ by splitting $A = M - N$ into two parts,\n",
    "* $M \\in \\R^{m \\times m}$ which is nonsingular and computationally cheap to invert\n",
    "* $N \\in \\R^{m \\times m}$ which is harder to work with\n",
    "\n",
    "Let $x^* \\in \\R^m$ be the exact solution to $A x^* = b$.  Then\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    Ax^* = (M-N)x^* &= b \\\\\n",
    "    Mx^* &= Nx^* + b \\\\\n",
    "     x^* &= M^{-1}(Nx^* + b)\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "Therefore, $x^*$ is a fixed point of $\\Phi(x) = M^{-1}(Nx + b)$.  We'll try to use **fixed point iteration** to approximate $x^*$:\n",
    "\n",
    "$$\n",
    "x_{n+1} = \\Phi(x_n) = M^{-1}(Nx + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "Consider the splitting $A = D + L + U$ where\n",
    "* $D$ is the diagonal of $A$\n",
    "* $U$ is the above-diagonal part of $A$\n",
    "* $L$ is the below-diagonal part of $A$\n",
    "\n",
    "The *classical* iterative methods use different combinations of the above matrices\n",
    "* **Jacobi:** $A = D + (U+L)$ where $M = D$ and $N = -(U+L)$\n",
    "* **Gauss-Seidel:** $A = (D+U) + L$ where $M=D+U$ and $N=-L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem - spectral radius\n",
    "\n",
    "Want to solve: $Ax = b$. Fixed point iteration: split $A = M - N$, where $M$ is easy-to-invert, $N$ is aribtrary. The **fixed point iteration** tries to find approximate $x^*$ via update:\n",
    "$$\n",
    "x_{t+1} = \\Phi(x_t) = M^{-1}(Nx_t + b)\n",
    "$$\n",
    "\n",
    "Recall that the *spectral radius* of a matrix $M$ is $\\rho(M) := \\max\\{|\\lambda_1(M)|, \\ldots, |\\lambda_n(M)|\\}$.\n",
    "\n",
    "**(Part A)**: Show that if $\\rho(M^{-1} N) > 1$, then the fixed point scheme above diverges for some initial $x_0$. \n",
    "\n",
    "*Hint*: assume that $b = 0$, and consider the largest eigenvector of $M^{-1}N$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution\n",
    "\n",
    "Let $x_0 = v$ be an eigenvector for $M^{-1}N$ whose eigenvalue $\\lambda$'s absolute value is larger than 1. If we assume that $b = 0$, then fixed point iteration is going to generate the point $x_t = (M^{-1}N)^t x_0 = \\lambda^t x_0$. The former clearly diverges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem - spectral radius\n",
    "\n",
    "Want to solve: $Ax = b$. Fixed point iteration: split $A = M - N$, where $M$ is easy-to-invert, $N$ is aribtrary. The **fixed point iteration** tries to find approximate $x^*$ via update:\n",
    "$$\n",
    "x_{t+1} = \\Phi(x_t) = M^{-1}(Nx_t + b)\n",
    "$$\n",
    "\n",
    "Recall that the *spectral radius* of a matrix $M$ is $\\rho(M) := \\max\\{|\\lambda_1(M)|, \\ldots, |\\lambda_n(M)|\\}$.\n",
    "\n",
    "**(Part B)**: Show that if $\\rho(M^{-1} N) < 1$, the fixed point scheme above converges to the solution $x$ for any initial $x_0$.\n",
    "\n",
    "*Hint*: Let's say $x^*$ satisfies the linear system, $Ax^* = b$. You want to look at how the error terms $\\text{err}_t := x_t - x^*$ evolve after each iteration. Can you show $\\|\\text{err}_t\\|$ is shrinking from round to round?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution\n",
    "\n",
    "If you consider the error term $\\text{err}_t := x_t - x^*$, then it is easy to check that\n",
    "$$\\text{err}_{t+1} = (M^{-1}N) \\text{err}_t$$\n",
    "Now let us see what happens to the norm of $\\|\\text{err}_{t+1}\\|$:\n",
    "$$\\|\\text{err}_{t+1}\\| = \\|(M^{-1}N) \\text{err}_t\\| = \\|\\text{err}_t\\| \\|(M^{-1}N) \\frac{\\text{err}_t}{\\|\\text{err}_t\\|}\\| \\leq \\|\\text{err}_t\\| \\max_{\\|z\\| = 1} \\|M^{-1}N z\\|$$\n",
    "However, $\\max_{\\|z\\| = 1} \\|M^{-1}N z\\|$ is indeed the spectral radius $\\rho(M^{-1}N)$, hence we have shown that\n",
    "\\[\n",
    "\\|\\text{err}_{t+1}\\| \\leq \\|\\text{err}_t\\| \\rho(M^{-1}N),\n",
    "\\]\n",
    "which implies that the error term is always shrinking at a geometric rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem\n",
    "\n",
    "> TODO:  Jacobi/Gauss Seidel converge whenever $A$ is strictly diagonally dominant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "> TODO:  Damped Jacobi $M = \\omega^{-1} D$ and $N = \\omega^{-1} D - A$\n",
    "> Find the eigvals of the iteration matrix\n",
    "> Show damped jacobi can converge even when Jacobi doesn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "> TODO:  Implement Jacobi/Gauss Seidel for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
